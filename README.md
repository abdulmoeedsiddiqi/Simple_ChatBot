# Simple_ChatBot
ğŸ“˜ Introduction to AI â€“ RAG with LangChain + Google Gemini

This project implements a Retrieval-Augmented Generation (RAG) pipeline using LangChain and Google Gemini.

It uses a JSON file ğŸ“„ "Introduction_to_AI.json" as the knowledge base and allows you to ask AI-related questions such as:

What is supervised learning?

What is meant by labeled data?

The system retrieves the relevant context from the file and generates accurate answers using Gemini.

ğŸš€ Features

âœ… Load and process JSON data
âœ… Chunk documents for better LLM understanding
âœ… Create embeddings using GoogleGenerativeAIEmbeddings
âœ… Store embeddings in ChromaDB for efficient retrieval
âœ… Use a Retriever to fetch top relevant chunks
âœ… Build a RetrievalQA Chain with Gemini
âœ… Custom Prompt Template to restrict answers only to available context

âš™ï¸ Installation

Install dependencies:

pip install -U langchain langchain-community langchain-core langchain-google-genai chromadb

ğŸ”‘ Environment Setup

Set your Google Gemini API Key:

import os
os.environ["GOOGLE_API_KEY"] = "your_api_key_here"


Replace "your_api_key_here" with your actual Gemini API key.

ğŸ“‚ Project Workflow

ğŸ“Œ Step 1 â€“ Load JSON file

from langchain_community.document_loaders import JSONLoader
file_path = "/content/Introduction_to_Ai.json"


ğŸ“Œ Step 2 â€“ Chunk the data

from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)


ğŸ“Œ Step 3 â€“ Generate embeddings

from langchain_google_genai import GoogleGenerativeAIEmbeddings
embedder = GoogleGenerativeAIEmbeddings(model="models/embedding-001")


ğŸ“Œ Step 4 â€“ Store in ChromaDB

from langchain_community.vectorstores import Chroma
vector_db = Chroma.from_documents(documents=doc_chunks, embedding=embedder)


ğŸ“Œ Step 5 â€“ Set up Retriever + LLM

from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
retriever = vector_db.as_retriever(search_kwargs={"k": 3})
gemini_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", temperature=0.2)


ğŸ“Œ Step 6 â€“ Custom Prompt + QA Chain

from langchain.prompts import PromptTemplate
rag_prompt = """
You are an expert on AI. Use the context below to answer questions.
Context: {context}
Question: {question}
Answer:
"""
custom_prompt = PromptTemplate(template=rag_prompt, input_variables=["context", "question"])
qa_chain = RetrievalQA.from_chain_type(llm=gemini_llm, retriever=retriever, chain_type_kwargs={"prompt": custom_prompt})


ğŸ“Œ Step 7 â€“ Ask Questions

def Machine_Learning_Question(question):
    response = qa_chain.invoke({"query": question})
    print(response['result'])

questions = [
    "What is supervised learning?",
    "What is meant by labeled data?"
]

for q in questions:
    Machine_Learning_Question(q)

ğŸ§  Example Output
Question: What is supervised learning?
Answer: Supervised learning is a type of machine learning where the model is trained using labeled data.

ğŸ“Œ Tech Stack

LangChain â€“ RAG framework

Google Gemini API â€“ LLM & embeddings

ChromaDB â€“ Vector database

Python â€“ Implementation language

ğŸŒŸ Future Improvements

Add support for multiple datasets

Build a user-friendly front-end for querying

Enhance prompt engineering for better accuracy

ğŸ™Œ Contribution

Pull requests are welcome! If youâ€™d like to suggest features or report issues, please open an issue.

ğŸ“œ License

This project is licensed under the MIT License.
